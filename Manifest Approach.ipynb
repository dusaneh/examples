{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title: Manafest pre-processing approach\n",
    "#Author: Dusan Bosnjakovic / 03/05/2018\n",
    "#Desc: This approach takes in a sample data set, generates data types for user confirmation of treatment\n",
    "#    and lets the user specify whether variables are features/predictors or labels. Finally, the user can\n",
    "#    specify what type of a lable is it (continuous, multi-class or binary). Intermediate files are saved\n",
    "#    and the second stage can take in the sample files along with the manifest file.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import qgrid\n",
    "import sys\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "from datetime import date\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import MinMaxScaler#may not be used\n",
    "\n",
    "############################\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    " \n",
    "df = pd.read_excel('C:/Users/family/Pipeline/SHARE Data Set 2014 070218db_SM.xlsx', sheet_name='Refined Data3.0')\n",
    "df_orig = df\n",
    "\n",
    "projName='test2'\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "    #display(hx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\family\\Anaconda3\\envs\\LC\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "C:\\Users\\family\\Anaconda3\\envs\\LC\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\family\\Anaconda3\\envs\\LC\\lib\\site-packages\\pandas\\core\\generic.py:5430: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\family\\Anaconda3\\envs\\LC\\lib\\site-packages\\ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\family\\Anaconda3\\envs\\LC\\lib\\site-packages\\ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\family\\Anaconda3\\envs\\LC\\lib\\site-packages\\ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df_orig\n",
    "df=df[df['term']==' 36 months']\n",
    "yearOfDataset = 2014\n",
    "\n",
    "\n",
    "df['grade'] = np.where(df.grade == 'A', 1, \n",
    "              np.where(df.grade == \"B\", 2, \n",
    "              np.where(df.grade == \"C\", 3, \n",
    "              np.where(df.grade == \"D\", 4, \n",
    "              np.where(df.grade == \"E\", 5, \n",
    "              np.where(df.grade == \"F\", 6, \n",
    "              np.where(df.grade == \"G\", 7, -10)))))))\n",
    "\n",
    "df['empLength'] = np.where(df.empLength == '10+ years', 15, \n",
    "              np.where(df.empLength == \"1 year\", 1, \n",
    "              np.where(df.empLength == \"2 years\", 2, \n",
    "              np.where(df.empLength == \"3 years\", 3, \n",
    "              np.where(df.empLength == \"4 years\", 4, \n",
    "              np.where(df.empLength == \"5 years\", 5,  \n",
    "              np.where(df.empLength == \"6 years\", 6, \n",
    "              np.where(df.empLength == \"7 years\", 7, \n",
    "              np.where(df.empLength == \"8 years\", 8, \n",
    "              np.where(df.empLength == \"9 years\", 9,\n",
    "              np.where(df.empLength == \"< 1 year\", 0, -10)))))))))))\n",
    "\n",
    "df['bcutil'].fillna(-1000, inplace=True)\n",
    "df['bcOpenToBuy'].fillna(-1000, inplace=True)\n",
    "df['mthsSinceLastDelinq'].fillna(3000, inplace=True)\n",
    "df['mthsSinceLastRecord'].fillna(3000, inplace=True)\n",
    "df['mthssincelastmajorderog'].fillna(3000, inplace=True)\n",
    "df['moSinOldIlAcct'].fillna(3000, inplace=True)\n",
    "df['mthsSinceRecentBc'].fillna(3000, inplace=True)\n",
    "df['mthsSinceRecentBcDlq'].fillna(3000, inplace=True)\n",
    "df['mthsSinceRecentRevolDelinq'].fillna(3000, inplace=True)\n",
    "df['numTl120dpd2m'].fillna(0, inplace=True)## prob impute median\n",
    "df['percentBcGt75'].fillna(-100, inplace=True)## prob impute median\n",
    "df['revolUtil'].fillna(-100, inplace=True)## prob impute median\n",
    "\n",
    "\n",
    "df['FreeCashFlow'] = np.where(df.annualInc == 0, 1*(df.annualInc/12)-((df.annualInc*df.dti)/1200),\n",
    "              np.where(df.annualInc <=25000, 0.97*(df.annualInc/12)-((df.annualInc*df.dti)/1200), \n",
    "              np.where(df.annualInc <50000, 0.92*(df.annualInc/12)-((df.annualInc*df.dti)/1200), \n",
    "              np.where(df.annualInc <100000, 0.88*(df.annualInc/12)-((df.annualInc*df.dti)/1200), \n",
    "              np.where(df.annualInc <200000, 0.82*(df.annualInc/12)-((df.annualInc*df.dti)/1200), \n",
    "              np.where(df.annualInc <500000, 0.72*(df.annualInc/12)-((df.annualInc*df.dti)/1200), \n",
    "              np.where(df.annualInc >=500000, 0.69*(df.annualInc/12)-((df.annualInc*df.dti)/1200),-10000)))))))\n",
    "\n",
    "df['InstlToFCF'] = df.installment/df.FreeCashFlow\n",
    "\n",
    "\n",
    "df['earliestCrLine_alt'] = yearOfDataset-pd.to_datetime(df['earliestCrLine']).dt.year\n",
    "\n",
    "#impute\n",
    "#bcOpenToBuy\n",
    "#bcutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "\n",
    "maximumMissingness=0.2 #usually put to 0.2\n",
    "minimumSampleForFeatureSelection=300000 #good number is 50000 for performance, but depends on baseline/n/m\n",
    "minimumVarianceThreshold=0# default is 0.001\n",
    "\n",
    "featureSelectionRTreeN=70\n",
    "cv=False#set to true for more thorough feature evaluation\n",
    "testProp =0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: only treatment and class columns changes will be saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe68ca199ff44cb086bf14317a4bf051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': True, 'defauâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hx=pd.DataFrame(data=df.dtypes, index=None,columns=['d'])#,columns=['var','dTypes']\n",
    "#hx.dtypes\n",
    "d={'types':df.dtypes.astype('str'),'vars':hx.index.values.astype('str')}\n",
    "e=pd.DataFrame(d)\n",
    "#e['w']= np.where(e['types']=='float64', 'yes', 'no')\n",
    "#e['ee']= 4\n",
    "#e[\"types2\"] = e[\"types\"].astype('category')\n",
    "\n",
    "#print(e.dtypes)\n",
    "#print(e[\"types2\"].cat.categories.tolist())\n",
    "#print(e.types.unique())\n",
    "###################\n",
    "catVals=e[(e['types']=='category') | (e['types']=='object')].index.values\n",
    "distinctVals = pd.DataFrame(df[catVals].T.apply(lambda x: x.nunique(), axis=1)).rename(columns={0: 'distinctVals'})\n",
    "#print(distinctVals)\n",
    "#manifest = manifest.merge(distinctVals, left_index=True,right_index=True, how='left')\n",
    "e['distinctVals']=distinctVals\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "def treatmentRFx(row):\n",
    "    if row['types'] == 'object':\n",
    "        if row['distinctVals'] >1000:\n",
    "            return 'text'\n",
    "        elif row['distinctVals'] ==1:\n",
    "            return 'Ignore'\n",
    "        else:\n",
    "            return 'categorical'\n",
    "    elif row['types'] =='float64':\n",
    "        return 'numeric' \n",
    "    elif row['types'] =='datetime64[ns]':\n",
    "        return 'Ignore' \n",
    "    elif row['types'] =='int64':\n",
    "        return 'numeric' \n",
    "    elif row['types'] =='int32':\n",
    "        return 'numeric' \n",
    "    elif row['types'] =='category':\n",
    "        return 'categorical' \n",
    "    else:\n",
    "        return 'Ignore'\n",
    "\n",
    "def classRFx(row):\n",
    "    if (row['types'] == 'datetime64[ns]') | (row['treatment'] == 'text'):\n",
    "        return 'Ignore'\n",
    "    elif row['vars'] in (['label'],['target'],['response']):\n",
    "        return 'IDColumnIgnore'\n",
    "    elif 'ID' in row['vars']:\n",
    "        return 'IDColumnIgnore'\n",
    "    else:\n",
    "        \n",
    "        return 'Predictor'#Predictor\n",
    "    \n",
    "treatment = e.apply(treatmentRFx, axis=1).astype('category')\n",
    "treatment.cat.set_categories(['text','numeric','Ignore','categorical'],inplace=True)\n",
    "e[\"treatment\"] =treatment \n",
    "\n",
    "#class = e.apply(classRFx, axis=1).astype('category')\n",
    "e['class'] = e.apply(classRFx, axis=1).astype('category')\n",
    "#e['class']='Predictor'\n",
    "#e['class']=e['class'].astype('category')\n",
    "#classes2 = pd.Series(pd.Categorical(['Predictor']))\n",
    "\n",
    "#type2=\n",
    "e['class'].cat.set_categories(['IDColumnIgnore','Ignore','Predictor','LabelRegress','LabelBinary','LabelMultiClass'],inplace=True)\n",
    "#print(classes)\n",
    "\n",
    "\n",
    "e=e.drop(['vars'], axis=1)\n",
    "\n",
    "#e.w=2\n",
    "#print(1 if true else 0)\n",
    "\n",
    "inputSetup=qgrid.show_grid(e,\n",
    "                grid_options={'fullWidthRows': True,\n",
    "                              'syncColumnCellResize': True,\n",
    "                              'forceFitColumns': True,\n",
    "                              'rowHeight': 40,\n",
    "                              'enableColumnReorder': True,\n",
    "                              'enableTextSelectionOnCells': True,\n",
    "                              'editable': True})\n",
    "\n",
    "#widget=qgrid.show_grid(e)\n",
    "print('NOTE: only treatment and class columns changes will be saved')\n",
    "inputSetup\n",
    "\n",
    "#widget.get_changed_df()\n",
    "#type(hx.dtypes)\n",
    "#hx.dtypes.values\n",
    "#hz=pd.DataFrame(data=hx.index.values, index=None,columns=['x'])\n",
    "#h2=pd.concat([hz,hx],axis=1)\n",
    "#hz\n",
    "#hx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have chosen a binary classification model to predict: ['DietzBool']\n",
      "Data frame length is: 9593\n",
      "You have selected 71 predictors\n"
     ]
    }
   ],
   "source": [
    "manifest=inputSetup.get_changed_df()\n",
    "manifest=manifest.drop(['distinctVals'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#die;\n",
    "\n",
    "if len(manifest[manifest['class']=='Predictor'])==0:\n",
    "    print('ERROR: Please select at least 1 predictor in the \"class\" column')\n",
    "elif len([manifest.iloc[i,j] for i,j in zip(*np.where(manifest.applymap(lambda x: x == '')))])>0:\n",
    "    print('ERROR: There are missing values in the manifest')\n",
    "elif len([manifest.iloc[i,j] for i,j in zip(*np.where(pd.isnull(manifest)))])>0:\n",
    "    print('ERROR: There are NaN values in the manifest')\n",
    "    \n",
    "\n",
    "if len(manifest[manifest['class'].str.contains(\"Label\")])==0:\n",
    "    print('ERROR: Please select 1 label type in the \"class\" column')\n",
    "elif len(manifest[manifest['class'].str.contains(\"Label\")])>1:\n",
    "    print('ERROR: You have selected more than one label type in the \"class\" column. Only 1 is allowed')\n",
    "elif len(manifest[manifest['class'].str.contains(\"LabelRegress\")])>0:\n",
    "    regVar = manifest[manifest['class'].str.contains(\"LabelRegress\")].index.values\n",
    "    print(\"You have chosen a regression model to predict: \"+str(regVar))\n",
    "    regressVar = manifest[manifest['class'].str.contains(\"LabelRegress\")].index.values\n",
    "    dtype=manifest[manifest['class'].str.contains(\"LabelRegress\")]['types']\n",
    "    dtype=dtype[0]\n",
    "    if(dtype!='float64' and dtype!='int64'):\n",
    "        print('ERROR: You have selected a regression label but the label values aren\\'t not numeric, but rather '+str(dtype))\n",
    "elif len(manifest[manifest['class'].str.contains(\"LabelBinary\")])>0:\n",
    "    binVar = manifest[manifest['class'].str.contains(\"LabelBinary\")].index.values\n",
    "    print(\"You have chosen a binary classification model to predict: \"+str(binVar))\n",
    "    if df[binVar].nunique()[0]!=2:\n",
    "        print('ERROR: You have selected a binary label but the number of unique values for that label not 2, but rather '+str(df[binVar].nunique()[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Data frame length is: '+str(len(df)))\n",
    "print('You have selected '+str(len(manifest[manifest['class']=='Predictor']))+' predictors')\n",
    "if len(manifest[manifest['class'].str.contains(\"LabelMultiClass\")])>0:\n",
    "    mcVar = manifest[manifest['class'].str.contains(\"LabelMultiClass\")].index.values\n",
    "    print(\"You have chosen a multiclass classification model to predict: \"+str(mcVar))\n",
    "    multiLabelVar = manifest[manifest['class'].str.contains(\"LabelMultiClass\")].index.values\n",
    "    if df[multiLabelVar].nunique()[0]>100:\n",
    "        print('NOTICE: You have selected a multiclass label with high number of unique values: '+str(df[multiLabelVar].nunique()[0]))\n",
    "    else:\n",
    "        print('Your multiclass label has this many unique values: '+str(df[multiLabelVar].nunique()[0]))\n",
    "    \n",
    "        \n",
    "\n",
    "#np.where(pd.isnull(df))    \n",
    "\n",
    "\n",
    "#np.where(manifest.applymap(lambda x: x == ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "#favorite_color = { \"lion\": \"yellow\", \"kitty\": \"red\" }\n",
    "pickle.dump( manifest, open( projName+\"_manifest.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text columns will not be selected for this evaluation.\n",
      "dropped 1 rows due to nas\n",
      "0\n",
      "9592\n",
      "9592\n",
      "9592\n",
      "classifying...\n",
      "AUC: 0.968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "missingTestSet=manifest[(manifest['class']=='Predictor') | (manifest['class'].isin(['Predictor','LabelBinary','LabelMultiClass','LabelRegress']))]\n",
    "missingTestSet.index.values\n",
    "reducedDF=df[missingTestSet.index.values]\n",
    "missingStats=round((len(reducedDF.index)-reducedDF.count())/len(df),3)\n",
    "\n",
    "\n",
    "labCol = manifest[manifest['class'].isin([\"LabelRegress\",'LabelBinary',\"LabelMultiClass\"])].index.values\n",
    "labType = manifest[manifest['class'].isin([\"LabelRegress\",'LabelBinary',\"LabelMultiClass\"])]['class']\n",
    "\n",
    "print('Text columns will not be selected for this evaluation.')\n",
    "\n",
    "#die\n",
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "######################\n",
    "#univariate stats\n",
    "if (len(manifest[manifest['class'].isin([\"LabelRegress\",'LabelBinary',\"LabelMultiClass\"])]))==1:\n",
    "    nonMissingDF = df[missingStats[missingStats<20].index.values]\n",
    "    ValidPredictors=manifest[(manifest['class']=='Predictor') & ((manifest['treatment']=='numeric') | (manifest['treatment']=='categorical'))]\n",
    "    XVars = intersection(ValidPredictors.index.values,missingStats[missingStats<maximumMissingness].index.values)#intersects numeric predictors and non missing values\n",
    "    #print(intersection(XVars.index.values,missingStats[missingStats<20].index.values))\n",
    "    #die;\n",
    "    uvDF=nonMissingDF[XVars]\n",
    "    uvDF2 = uvDF.dropna()\n",
    "    print(\"dropped \"+str(len(uvDF)-len(uvDF2))+\" rows due to nas\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #uvDF2=uvDF2.sample(n=min(minimumSampleForFeatureSelection,len(uvDF2)))\n",
    "\n",
    "    #type(uvDF2)\n",
    "    X = uvDF2.values\n",
    "   \n",
    "    Y=df[labCol].values\n",
    "    Z = pd.concat([uvDF2, df[labCol]], axis=1, join='inner')\n",
    "    Z = Z.dropna()\n",
    "    \n",
    "    X = pd.get_dummies(Z[XVars])\n",
    "    Y = Z[labCol]\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    ################### text treatment start\n",
    "    ############################################################################\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import re\n",
    "\n",
    "    def tokenizer(text):\n",
    "        if text:\n",
    "            result = re.findall('[a-z]{2,}', text.lower())\n",
    "        else:\n",
    "            result = []\n",
    "        return result\n",
    "\n",
    "    vect = TfidfVectorizer(tokenizer=tokenizer, stop_words='english',token_pattern=r'\\w{2,}'\n",
    "                           ,ngram_range=(1, 2)\n",
    "                           ,max_df=0.5\n",
    "                           , min_df=.0005,max_features=100\n",
    "                          )\n",
    "\n",
    "    TextPreds=manifest[(manifest['class']=='Predictor') & (manifest['treatment']=='text')].index.values\n",
    "    reducedTextDF=df[TextPreds]\n",
    "    #reducedTextDF=reducedTextDF.iloc[Z.uvDF2]\n",
    "    \n",
    "    textFeatures = pd.DataFrame()\n",
    "    for i in range(0, len(TextPreds)):\n",
    "\n",
    "        #i=0\n",
    "\n",
    "        reducedSingleTextDF = reducedTextDF[TextPreds[i]].str.replace('\\d+', '')#remove numbers\n",
    "        reducedSingleTextDF.fillna(value='SpecialTextMissingValue', inplace=True)\n",
    "\n",
    "        X1 = (reducedSingleTextDF).values\n",
    "\n",
    "        X_train_vect = vect.fit_transform(X1)\n",
    "\n",
    "        #X_train_vect.to_dense()\n",
    "        X_train_vectSDF = pd.SparseDataFrame(X_train_vect)#.fillna(0, inplace=True)\n",
    "        X_train_vectSDF = X_train_vectSDF.fillna(0)\n",
    "\n",
    "        cols = [TextPreds[i]+'_'+str(s) for s in list(X_train_vectSDF)]\n",
    "\n",
    "        X_train_vectDF = pd.DataFrame(X_train_vect.todense(),columns=cols,index=reducedTextDF.index)\n",
    "\n",
    "        textFeatures = pd.concat([textFeatures, X_train_vectDF], axis=1, sort=False)\n",
    "\n",
    "    ############################################################################\n",
    "    ################### text treatment end\n",
    "    ############################################################################\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(textFeatures.isnull().values.any())Z = pd.concat([uvDF2, df[labCol]], axis=1, join='inner')\n",
    "    print(len(textFeatures))\n",
    "    print(len(X))\n",
    "    print(len(Y))\n",
    "    #print(textFeatures.head())\n",
    "    #print(X.head())\n",
    "    #print(Y.head())\n",
    "    \n",
    "    if(len(textFeatures)>0):\n",
    "        X = pd.concat([textFeatures, X], axis=1, sort=False, join='inner')\n",
    "    #print(len(pd.concat([textFeatures, X], axis=1, sort=False, join='inner')))\n",
    "    \n",
    "    #X=X.append(textFeatures)\n",
    "    #print(X.isnull().values.any())\n",
    "    #print(X.shape)\n",
    "    print(len(X))\n",
    "    \n",
    "\n",
    "    colNames = list(X)\n",
    "    \n",
    "    \n",
    "    #print(colNames)\n",
    "    \n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X = pd.DataFrame(scaler.fit_transform(X),columns=colNames)\n",
    "    \n",
    "    \n",
    "    #result = df1.append(df2)\n",
    "\n",
    "    \n",
    "    \n",
    "    #########################\n",
    "    \n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    def variance_threshold_selector(data, threshold=0):\n",
    "        selector = VarianceThreshold(threshold)\n",
    "        selector.fit(data)\n",
    "        return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "    vts=variance_threshold_selector(X,threshold=minimumVarianceThreshold)\n",
    "    X = X[list(vts)]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, Y, test_size=testProp, random_state=None)\n",
    "    \n",
    "\n",
    "    #list(zip(list(vts)))\n",
    "    #list(vts)\n",
    "    #len(list(X[list(vts)]))\n",
    "    #len(list(X))\n",
    "    #print(list(X[list(vts)]))\n",
    "    \n",
    "    #Low var test\n",
    "    \n",
    "    #numericDF = df[manifest[(manifest['class']=='Predictor') & (manifest['treatment']=='numeric')].index.values]\n",
    "    #sel = VarianceThreshold(threshold=(.5 * (1 - .5)))\n",
    "    #sel.fit_transform(X)\n",
    "    #numericDF.head()\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create the random grid\n",
    "\n",
    "    \n",
    "#{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4\n",
    "#, 'max_features': 'auto', 'max_depth': None, 'bootstrap': True}\n",
    "\n",
    "    #print(list(X))\n",
    "    if(labType.values==\"LabelRegress\"):\n",
    "        print('regressing...')\n",
    "        \n",
    "        \n",
    "        if(cv==True):\n",
    "            from sklearn.model_selection import RandomizedSearchCV\n",
    "                # Number of trees in random forest\n",
    "            n_estimators = [int(x) for x in np.linspace(start = 200, stop = 400, num = 3)]\n",
    "            # Number of features to consider at every split\n",
    "            max_features = ['sqrt']\n",
    "            # Maximum number of levels in tree\n",
    "            max_depth = [3,5]#[int(x) for x in np.linspace(2, 5, num = 3)]\n",
    "            max_depth.append(None)\n",
    "            # Minimum number of samples required to split a node\n",
    "            min_samples_split = [4, 5, 6]\n",
    "            # Minimum number of samples required at each leaf node\n",
    "            min_samples_leaf = [3, 4,5]\n",
    "            # Method of selecting samples for training each tree\n",
    "            bootstrap = [False]\n",
    "            \n",
    "            random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "            rf = RandomForestRegressor()\n",
    "            cvfit = RandomizedSearchCV(estimator = rf, param_distributions = random_grid\n",
    "                                 , n_iter = 15, cv = 7, verbose=2, random_state=None, n_jobs = -1)\n",
    "            cvfit.fit(X_train, y_train.values.ravel())\n",
    "            print(cvfit.best_params_)\n",
    "            clf = cvfit.best_estimator_\n",
    "            \n",
    "        else:\n",
    "            clf = RandomForestRegressor(max_depth=5,random_state=None,n_estimators=30,min_samples_leaf=4\n",
    "                                        ,min_samples_split= 5,max_features='auto',bootstrap= True)\n",
    "\n",
    "\n",
    "        clf.fit(X_train, y_train.values.ravel())\n",
    "        #print(clf.best_params_)\n",
    "                                       \n",
    "        #clf.fit(X, Y.values.ravel())\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        from math import sqrt\n",
    "        rms = sqrt(mean_squared_error(y_test.values.ravel(), clf.predict(X_test)))\n",
    "        print(\"RMSQ: \"+str(round(rms,3)))\n",
    "    else:\n",
    "        print('classifying...')\n",
    "        clf = RandomForestClassifier(max_depth=2, random_state=0,n_estimators=featureSelectionRTreeN)\n",
    "        clf.fit(X, Y.values.ravel())\n",
    "        if(labType.values==\"LabelBinary\"):\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "            auc = roc_auc_score(Y.values.ravel(), clf.predict(X))\n",
    "            print(\"AUC: \"+str(round(auc,3)))\n",
    "        elif(labType.values==\"LabelMultiClass\"):\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            #lrap = label_ranking_average_precision_score(Y.values.ravel(), clf.predict(X))\n",
    "            acc=accuracy_score(Y.values.ravel(), clf.predict(X))\n",
    "            print(\"Accuracy: \"+str(round(acc,3)))\n",
    "        \n",
    "    \n",
    "    #print(zip(clf.feature_importances_,list(X)))\n",
    "    #print(pd.concat(list(X), clf.feature_importances_, axis=1, join='inner'))\n",
    "    #print((X[['home_ownership']]))\n",
    "    #print(pd.get_dummies(X[['home_ownership','installment']]))\n",
    "    \n",
    "    bestModel = clf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#favorite_color = { \"lion\": \"yellow\", \"kitty\": \"red\" }\n",
    "pickle.dump( bestModel, open( projName+\"_bestModelForFeatureEng.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#favorite_color = { \"lion\": \"yellow\", \"kitty\": \"red\" }\n",
    "#pickle.dump( optimized_GBM.best_estimator_, open( \"xgb.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureImpVal will be \"NaN\" if 1) the variable is not a predictor, 2) the missing rate is below the given value (default=0.2) or 3) if the given varience threshold (default=0.001) wasnt reached\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a2bfbdeb2b4bf584d4e5d00f3475a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': True, 'defauâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(list(X))\n",
    "\n",
    "#bestModel\n",
    "\n",
    "#print(clf.feature_importances_)\n",
    "#clf.best_estimator_\n",
    "#print(list(zip(list(X), clf.feature_importances_)))\n",
    "varImp=pd.DataFrame(list(zip(list(X), bestModel.feature_importances_)),columns=('vars','featureImpVal'))\n",
    "varImp.sort_values(by=['featureImpVal'], ascending=False)\n",
    "\n",
    "#print(manifest[manifest['treatment']=='categorical'])\n",
    "#varImp[varImp['vars'].str.contains(\"emp_title\"+\"_\")]\n",
    "#cats=manifest[manifest['treatment']=='categorical'].index\n",
    "#print(list(Z[XVars]))\n",
    "#print(varImp[varImp['vars'].isin(list(Z[XVars]))])\n",
    "\n",
    "#for index,row in varImp.iterrows():\n",
    "    #if  row['vars'] in manifest[manifest['treatment']=='categorical'].index.values+\"_\":\n",
    "    #print(1)\n",
    "\n",
    "featureImp = pd.DataFrame(varImp[varImp['vars'].isin(list(Z[XVars]))])\n",
    "\n",
    "\n",
    "for index,row in manifest[((manifest['treatment']=='categorical') | (manifest['treatment']=='text')) & (manifest['class']=='Predictor')].iterrows():\n",
    "    #print(varImp[varImp['vars'].str.contains(str(row.name)+\"_\")]['featureImpVal'].mean())\n",
    "    #featureImp[row.name][varImp[varImp['vars'].str.contains(str(row.name)+\"_\")]['featureImpVal'].mean()]\n",
    "    featureImp = featureImp.append({'vars': row.name,'featureImpVal': round(varImp[varImp['vars'].str.contains(str(row.name)+\"_\")]['featureImpVal'].mean(),7)}, ignore_index=True)\n",
    "    #print(str(row.name))\n",
    "#print(featureImp)\n",
    "   \n",
    "featureImp=featureImp.dropna()\n",
    "featureImp=featureImp.set_index('vars')\n",
    "\n",
    "#print(manifest[manifest['treatment']=='categorical']].name)\n",
    "#print(featureImp.sort_values(by=['featureImpVal'], ascending=False))\n",
    "#print(pd.DataFrame(missingStats))\n",
    "catVals=manifest[(manifest['treatment']=='categorical') & (manifest['class']=='Predictor')].index.values\n",
    "distinctVals = pd.DataFrame(df[catVals].T.apply(lambda x: x.nunique(), axis=1)).rename(columns={0: 'distinctVals'})\n",
    "\n",
    "#missingStats=pd.DataFrame(missingStats)#.columns(['missingRt'])\n",
    "\n",
    "left_merge = pd.DataFrame(missingStats).merge(featureImp, left_index=True,right_index=True, how='left')\n",
    "#print(left_merge)\n",
    "left_merge=left_merge.rename(columns={0: 'missingRate', 'featureImpVal': 'featureImpVal'})\n",
    "left_merge = left_merge.merge(distinctVals, left_index=True,right_index=True, how='left')\n",
    "left_merge = manifest.merge(left_merge, left_index=True,right_index=True, how='left')\n",
    "#left_merge.columns(['missingRate','featureImpVal'])\n",
    "\n",
    "\n",
    "def classTreatmentFx(row):\n",
    "    if ((row['class'] == 'Predictor') & (row['treatment'] == 'categorical')):\n",
    "        if row['distinctVals'] > 10:\n",
    "            return 'group by risk/size'\n",
    "        else:\n",
    "            return 'binarize'\n",
    "    else:\n",
    "        return 'ignore treatment'\n",
    "            \n",
    "    \n",
    "classTreatment = left_merge.apply(classTreatmentFx, axis=1).astype('category')\n",
    "classTreatment.cat.set_categories(['binarize','group by risk/size','assign risk score','ignore treatment'],inplace=True)\n",
    "left_merge[\"classTreatment\"] =classTreatment \n",
    "#print(left_merge)\n",
    "\n",
    "left_merge=left_merge.drop(['types'], axis=1)\n",
    "\n",
    "print('featureImpVal will be \"NaN\" if 1) the variable is not a predictor, 2) the missing rate is below the given value (default=0.2) or 3) if the given varience threshold (default=0.001) wasn''t reached')\n",
    "inputSetup2=qgrid.show_grid(left_merge)\n",
    "inputSetup2\n",
    "#print(pd.DataFrame(data=featureImp['featureImpVal'],index=featureImp['vars']))\n",
    "#Z = pd.concat([pd.DataFrame(missingStats), df[labCol]], axis=1, join='inner')\n",
    "#varImp[varImp['vars'] in manifest['treatment']=='categorical']]\n",
    "#varImp['emp_title' in varImp['vars']]\n",
    "#for index, row in varImp.iterrows():\n",
    "    #print(row['vars'], row['featureImpVal'])\n",
    "    #'ID' in row['vars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest2=inputSetup2.get_changed_df()\n",
    "pickle.dump(manifest2, open( projName+\"_manifestWFImp.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\family\\Anaconda3\\envs\\LC\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\family\\Anaconda3\\envs\\LC\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSQ: 0.002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\n",
    "parameters = {'max_depth': [3,5], 'min_child_weight': [1,3,5],'learning_rate': [0.1, 0.05, 0.01]\n",
    "              , 'n_estimators': [100, 300, 500], 'subsample': [0.7,0.8,0.9]\n",
    "              , 'colsample_bytree': [0.8, 1]}\n",
    "# optimized_GBM = GridSearchCV(XGBRegressor(ind_params), \n",
    "#                             cv_params, \n",
    "#                              scoring = 'neg_mean_absolute_error', cv = 10, n_jobs = -1)\n",
    "###############\n",
    "#parameters  = {'max_depth': [5], 'min_child_weight': [1],'learning_rate': [0.1], 'n_estimators': [100], 'subsample': [1]\n",
    "#              , 'colsample_bytree': [1]\n",
    "#              ,'gamma':[0],'max_delta_step':[0],'reg_alpha':[0], 'reg_lambda':[1], 'scale_pos_weight':[1],'silent':[1]\n",
    "#              ,'objective':['reg:linear'],}\n",
    "\n",
    "optimized_GBM = GridSearchCV(XGBRegressor()\n",
    "                             ,parameters ,scoring = 'neg_mean_absolute_error', cv = 5, n_jobs = -1)\n",
    "\n",
    "\n",
    "\n",
    "###############\n",
    "optimized_GBM.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = optimized_GBM.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test.values.ravel(), y_pred))\n",
    "print(\"RMSQ: \"+str(round(rms,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
